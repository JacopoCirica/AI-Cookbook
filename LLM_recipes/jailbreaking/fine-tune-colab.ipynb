{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020712cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reset our files for the new session\n",
    "# !rm -rf /content/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889cc0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is for Google colab enviroment:\n",
    "\n",
    "! pip install transformers datasets\n",
    "! pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # This opens a file picker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3654eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ðŸ”§ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f198f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"chinese_history_uncensored.jsonl\")\n",
    "print(dataset)  # Check it loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1102226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\" # Use model of your choice. Provide correct model path from Hugging Face\n",
    "\n",
    "# Download model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name,\n",
    "    max_seq_length=512,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Optimize memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a144d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "# It's used to clean up the dataset after tokenization and remove unnecessary columns\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b946e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"wandb\" is used for tracking and visualizing your machine learning experiments\n",
    "# \"Trainer\" tries to use W&B by default if it's installed\n",
    "# But it's not required unless you want to\n",
    "# In local dev environments, unless you manually \"pip install wandb\", it won't get triggered\n",
    "# import os\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,  # Train for 3 full passes\n",
    "    per_device_train_batch_size=4,  # Small batches for memory\n",
    "    gradient_accumulation_steps=4,  # Combines batches to act bigger\n",
    "    learning_rate=5e-5,  # Slow learning for accuracy\n",
    "    fp16=True,  # Speeds up on GPU\n",
    "    logging_steps=10,  # Check progress often\n",
    "    save_steps=100,  # Save regularly\n",
    "    save_total_limit=2,  # Keep only 2 saves\n",
    "    report_to=\"none\",  # Disables all logging integrations, including \"wandb\". So we don't need to install extra libraries\n",
    ")\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8,  # Optimize for GPU\n",
    ")\n",
    "\n",
    "# Create Trainer and train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# # Save the model (optional)\n",
    "# model.save_pretrained(\"./fine_tuned_model\")\n",
    "# tokenizer.save_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d78bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# # Reload the fine-tuned model from local storage (optional, only if model was previously saved)\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name=\"./fine_tuned_model\",  # model_name as a path\n",
    "#     max_seq_length=512,\n",
    "#     dtype=None,\n",
    "#     load_in_4bit=True,\n",
    "#     local_files_only=True,  # Ensures only local files are used\n",
    "# )\n",
    "\n",
    "# Inference optimization\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.eval()\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = \"Question: What happened during the Tiananmen Square protests in 1989?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Move inputs to GPU if available\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=5000, # Specifies the maximum number of tokens the model is allowed to generate\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "# Decode and clean response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "wrapped_response = textwrap.fill(response, width=100, break_long_words=False)\n",
    "\n",
    "print(\"\\nðŸ§  Model Response:\\n\")\n",
    "print(wrapped_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d88fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check base model answers\n",
    "import textwrap\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"  # Use model of your choice. Provide correct model path from Hugging Face\n",
    "\n",
    "# Load the base model and tokenizer using FastLanguageModel (without LoRA modifications)\n",
    "test_base_model, test_base_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name,\n",
    "    max_seq_length=512,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,  # Ensure this setting is for loading in 4-bit precision\n",
    ")\n",
    "\n",
    "# Inference optimization\n",
    "FastLanguageModel.for_inference(test_base_model)\n",
    "test_base_model.eval()\n",
    "\n",
    "# Define the prompt for testing\n",
    "prompt = \"Question: What happened during the Tiananmen Square protests in 1989?\"\n",
    "inputs = test_base_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Move inputs to GPU if available\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate and print the base model response\n",
    "with torch.no_grad():\n",
    "    base_outputs = test_base_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=5000, # Specifies the maximum number of tokens the model is allowed to generate\n",
    "        pad_token_id=test_base_tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "base_response = test_base_tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "base_wrapped_response = textwrap.fill(base_response, width=100, break_long_words=False)\n",
    "\n",
    "print(\"\\nðŸ§  Base Model (Preâ€“fine-tuning) Response:\\n\")\n",
    "print(base_wrapped_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
